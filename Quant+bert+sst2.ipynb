{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PraneelUJ/CS_203_Lab11/blob/main/Quant%2Bbert%2Bsst2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsIrX-AkvlkA",
        "outputId": "7c46b8b9-d5cf-4ef4-b0e4-d511dd79e10c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Model Quantization?\n",
        "Quantization is the process of reducing the numerical precision of model weights and/or activations, typically from 32-bit floating point (FP32) to lower-precision formats:\n",
        "- 8-bit integer (INT8)\n",
        "- 16-bit floating point (FP16)\n",
        "- Even lower precision formats like FP8 or INT4"
      ],
      "metadata": {
        "id": "wJhCok1U3Dp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Quantize?\n",
        "1. Reduced memory footprint\n",
        "2. Faster inference (As the model size is smaller).\n",
        "3. Better deployment on edge devices"
      ],
      "metadata": {
        "id": "y9NsnldP3KZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Gets Quantized?\n",
        "- **Weights**: The parameters learned during training\n",
        "- **Activations**: The outputs of each layer during inference\n",
        "- **Operations**: The computations themselves (e.g., matrix multiplications)"
      ],
      "metadata": {
        "id": "EDcN1mDq4ru7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"PyTorch_Quant_BERT_SST.py\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import psutil\n",
        "import json\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "import tempfile\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KJ23uCRzcBc",
        "outputId": "9a5a92c1-1a61-46d2-9b42-c467eb719f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SST-2 dataset from the Hugging Face datasets library\n",
        "print(\"Loading SST-2 dataset...\")\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to preprocess the dataset\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "# Preprocess the validation dataset for evaluation\n",
        "encoded_val_dataset = sst2_dataset['validation'].map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ1qbm48zehc",
        "outputId": "3000813f-38f2-44e0-843b-46bd2a0f336f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SST-2 dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "val_input_ids = torch.tensor(encoded_val_dataset['input_ids'], dtype=torch.long)\n",
        "val_attention_mask = torch.tensor(encoded_val_dataset['attention_mask'], dtype=torch.long)\n",
        "val_token_type_ids = torch.tensor(encoded_val_dataset['token_type_ids'], dtype=torch.long)\n",
        "val_labels = torch.tensor(encoded_val_dataset['label'], dtype=torch.long)"
      ],
      "metadata": {
        "id": "gzKyemM1zgZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create PyTorch Dataset and DataLoader\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_mask, val_token_type_ids, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=8)  # Using smaller batch size to avoid memory issues\n",
        "\n",
        "# Load pre-trained BERT model fine-tuned on SST-2\n",
        "print(\"Loading pre-trained BERT model...\")\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.eval()  # Set model to evaluation mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaRXWqYYziLk",
        "outputId": "11eee478-085a-4543-d274-a5e580816ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained BERT model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model parameter count and size\n",
        "def count_params(model):\n",
        "    \"\"\"Count the total parameters in a model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def estimate_model_size_mb(model):\n",
        "    \"\"\"Estimate the size of a model in MB based on parameter count.\"\"\"\n",
        "    total_params = count_params(model)\n",
        "    # Each parameter is 4 bytes (32 bits) for float32\n",
        "    size_bytes = total_params * 4\n",
        "    return size_bytes / (1024 * 1024)\n",
        "## TODO: A BETTER APPROACH TO FIND THE DATA-TYPE AND THEN COMPUTE THE SIZE. HERE WE ASSUME THE TYPE.\n",
        "\n",
        "# Measure original model size\n",
        "original_param_count = count_params(model)\n",
        "original_model_size_mb = estimate_model_size_mb(model)\n",
        "print(f\"Original BERT model parameter count: {original_param_count:,}\")\n",
        "print(f\"Estimated original BERT model size: {original_model_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2m1VKUEzkGF",
        "outputId": "d9bcd01f-784c-4f46-cf3f-a2c8113920d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original BERT model parameter count: 109,483,778\n",
            "Estimated original BERT model size: 417.65 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary directory to save models\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "original_model_path = os.path.join(temp_dir, \"bert_sst2_model.pt\")\n",
        "\n",
        "# Save the original model\n",
        "print(\"Saving the model...\")\n",
        "torch.save(model.state_dict(), original_model_path)\n",
        "\n",
        "# Get saved model size\n",
        "def get_file_size_mb(file_path):\n",
        "    \"\"\"Get the size of a file in MB.\"\"\"\n",
        "    return os.path.getsize(file_path) / (1024 * 1024)\n",
        "\n",
        "original_saved_size_mb = get_file_size_mb(original_model_path)\n",
        "print(f\"Saved model size: {original_saved_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSAz3G4LzlvC",
        "outputId": "dd993777-f998-4e9d-a603-05e6cbb87678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the model...\n",
            "Saved model size: 417.72 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure RAM usage\n",
        "def get_ram_usage():\n",
        "    \"\"\"Get current RAM usage in MB.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    return memory_info.rss / (1024 * 1024)"
      ],
      "metadata": {
        "id": "OkxYPbU0znlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate model accuracy\n",
        "def evaluate_model(model, dataloader, device='cpu', num_samples=50):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, token_type_ids, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)\n",
        "\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if total >= num_samples:\n",
        "                break\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "78pNkdWjzpL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to measure inference time\n",
        "def measure_inference_time(model, text, tokenizer, device='cpu', num_runs=10):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
        "\n",
        "    # Warm-up run\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "    # Measure inference time\n",
        "    inference_times = []\n",
        "    for _ in range(num_runs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "            _ = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
        "        inference_times.append(inference_time)\n",
        "\n",
        "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
        "    return avg_inference_time"
      ],
      "metadata": {
        "id": "IWx3W8rQzq_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict sentiment\n",
        "def predict_sentiment(model, text, tokenizer, device='cpu'):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    token_type_ids = inputs[\"token_type_ids\"].to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    inference_time = (time.time() - start_time) * 1000  # ms\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "    prediction = torch.argmax(probabilities, dim=1).item()\n",
        "    confidence = probabilities[0][prediction].item()\n",
        "\n",
        "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "\n",
        "    return sentiment, confidence, inference_time"
      ],
      "metadata": {
        "id": "csBpcdMuzs0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate baseline model performance\n",
        "print(\"\\nEvaluating original model...\")\n",
        "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = model.to(device)\n",
        "baseline_accuracy = evaluate_model(model, val_dataloader, device)\n",
        "print(f\"Baseline model accuracy: {baseline_accuracy:.4f}\")\n",
        "\n",
        "sample_text = \"This movie was fantastic, I really enjoyed it.\"\n",
        "baseline_inference_time = measure_inference_time(model, sample_text, tokenizer, device)\n",
        "print(f\"Baseline model average inference time: {baseline_inference_time:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by7EL3Myzuui",
        "outputId": "905c50c5-8436-416b-dc33-a267c4889703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating original model...\n",
            "Using device: cpu\n",
            "Baseline model accuracy: 0.4821\n",
            "Baseline model average inference time: 407.98 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Quantization Formats\n",
        "\n",
        "### FP32 (standard)\n",
        "- 32-bit floating point format\n",
        "- 1 bit: sign\n",
        "- 8 bits: exponent\n",
        "- 23 bits: mantissa (fractional part)\n",
        "- Used as default in most deep learning training\n",
        "\n",
        "### FP16 (half precision)\n",
        "- 16-bit floating point format\n",
        "- 1 bit: sign\n",
        "- 5 bits: exponent\n",
        "- 10 bits: mantissa\n",
        "- Reduces memory usage by 50% compared to FP32\n",
        "- Works well for models that don't need extreme precision\n",
        "\n",
        "### INT8 (quantized integer)\n",
        "- 8-bit integer format\n",
        "- Drastically reduces memory by 75% compared to FP32\n",
        "- Requires mapping floating point values to integers\n",
        "- Needs careful handling of dynamic range"
      ],
      "metadata": {
        "id": "DUkWPWAc5AaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Dynamic Quantization\n",
        "------------------------\n",
        "\n",
        "## What is Dynamic Quantization?\n",
        "\n",
        "Dynamic quantization is a post-training quantization technique that:\n",
        "- Converts weights from FP32 to INT8 (For this example) during model loading.\n",
        "- Keeps activations in floating point format\n",
        "- Computes the quantization parameters (scale, zero-point) on-the-fly during inference\n",
        "- Is the simplest form of quantization to apply in PyTorch\n",
        "\n",
        "### What gets quantized in Dynamic Quantization?\n",
        "- ✅ WEIGHTS: Converted from FP32 to INT8 (50-75% size reduction)\n",
        "- ❌ ACTIVATIONS: Remain in floating point\n",
        "- ✅ OPERATIONS: Linear operations (matrix multiplies) are performed with INT8 weights\n",
        "\n",
        "### Key Properties:\n",
        "- Applied after training (no retraining needed)\n",
        "- Accuracy impact is usually minimal for NLP models\n",
        "- Weights are quantized statically, calculations are done with 8-bit arithmetic\n",
        "- Currently only supported on CPU, not on CUDA (GPU)\n",
        "- Works best for models where weights are the main memory bottleneck\n",
        "\n",
        "\n",
        "The primary activations that get quantized are:\n",
        "- Intermediate layer outputs\n",
        "- Input tensors to linear/convolutional layers\n",
        "- Outputs of activation functions\n"
      ],
      "metadata": {
        "id": "-eV6s9EF9lDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the weight quantization:\n",
        "\n",
        "- Weights = [0.5, -0.9, 1.2]\n",
        "- Range: min = -0.9, max = 1.2\n",
        "- Scale = (1.2 - (-0.9)) / 255 ≈ 0.00824\n",
        "- Zero point = round(-min/scale) = round(0.9/0.00824) ≈ 109\n",
        "\n",
        "Converting weights to int8:\n",
        "\n",
        "- For 0.5: round((0.5 - (-0.9))/0.00824) ≈ round(170.5) = 171\n",
        "- For -0.9: round((-0.9 - (-0.9))/0.00824) ≈ round(0) = 0\n",
        "- For 1.2: round((1.2 - (-0.9))/0.00824) ≈ round(255) = 255\n",
        "\n",
        "Quantized weights: [171, 0, 255]\n",
        "For the activation quantization:\n",
        "\n",
        "- Input = [2.7, -1.3, 0.8]\n",
        "- Range: min = -1.3, max = 2.7\n",
        "- Scale = (2.7 - (-1.3)) / 255 ≈ 0.01569\n",
        "- Zero point = round(-min/scale) = round(1.3/0.01569) ≈ 83\n",
        "\n",
        "Converting activations to int8:\n",
        "\n",
        "- For 2.7: round((2.7 - (-1.3))/0.01569) ≈ round(255) = 255\n",
        "- For -1.3: round((-1.3 - (-1.3))/0.01569) ≈ round(0) = 0\n",
        "- For 0.8: round((0.8 - (-1.3))/0.01569) ≈ round(134) = 134\n",
        "\n",
        "Quantized activations: [255, 0, 134]\n",
        "\n",
        "The computation would then use these quantized values, applying the scale factors when converting back to floating point for the final result.\n",
        "\n",
        "This is the essence of dynamic quantization - *the activation scale is calculated per inference based on the actual input values, rather than using a pre-determined fixed scale.*"
      ],
      "metadata": {
        "id": "hH_9f72MaWfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DYNAMIC QUANTIZATION\n",
        "# ----------------------\n",
        "print(\"\\n1. Applying dynamic quantization...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Apply dynamic quantization to the model\n",
        "# This is post-training quantization\n",
        "dynamic_quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model,  # The model to quantize\n",
        "    {torch.nn.Linear},  # Specify which modules to quantize (Linear layers in BERT)\n",
        "    dtype=torch.qint8  # Quantization data type\n",
        ")\n",
        "\n",
        "dynamic_quant_time = time.time() - start_time\n",
        "print(f\"Dynamic quantization time: {dynamic_quant_time:.2f} seconds\")\n",
        "\n",
        "# Save the quantized model\n",
        "dynamic_quantized_path = os.path.join(temp_dir, \"bert_sst2_dynamic_quantized.pt\")\n",
        "torch.save(dynamic_quantized_model.state_dict(), dynamic_quantized_path)\n",
        "dynamic_quantized_size_mb = get_file_size_mb(dynamic_quantized_path)\n",
        "\n",
        "print(f\"Dynamic quantized model size: {dynamic_quantized_size_mb:.2f} MB\")\n",
        "print(f\"Size reduction: {(1 - dynamic_quantized_size_mb/original_saved_size_mb) * 100:.2f}%\")\n",
        "\n",
        "# Evaluate dynamic quantized model\n",
        "dynamic_quantized_model = dynamic_quantized_model.to(device)\n",
        "dynamic_quantized_accuracy = evaluate_model(dynamic_quantized_model, val_dataloader, device)\n",
        "print(f\"Dynamic quantized model accuracy: {dynamic_quantized_accuracy:.4f}\")\n",
        "\n",
        "dynamic_quantized_inference_time = measure_inference_time(dynamic_quantized_model, sample_text, tokenizer, device)\n",
        "print(f\"Dynamic quantized model average inference time: {dynamic_quantized_inference_time:.2f} ms\")\n",
        "print(f\"Inference speedup: {baseline_inference_time / dynamic_quantized_inference_time:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN_mzwmLzxNm",
        "outputId": "97f0251d-c299-4408-a54b-27d126296083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Applying dynamic quantization...\n",
            "Dynamic quantization time: 1.02 seconds\n",
            "Dynamic quantized model size: 173.09 MB\n",
            "Size reduction: 58.56%\n",
            "Dynamic quantized model accuracy: 0.5357\n",
            "Dynamic quantized model average inference time: 216.08 ms\n",
            "Inference speedup: 1.89x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Half Precision (FP16)?\n",
        "\n",
        "Half precision is not strictly quantization but a format conversion that:\n",
        "- Reduces 32-bit floating point (FP32) to 16-bit floating point (FP16)\n",
        "- Halves the memory footprint of weights and activations\n",
        "- Is well-supported on modern GPUs with tensor cores (like NVIDIA's)\n",
        "\n",
        "### What gets converted in FP16?\n",
        "- ✅ WEIGHTS: Converted from FP32 to FP16 (50% size reduction)\n",
        "- ✅ ACTIVATIONS: Also processed in FP16\n",
        "- ✅ OPERATIONS: Matrix multiplications use FP16 arithmetic\n",
        "\n",
        "### Key Properties:\n",
        "- Simple to implement in PyTorch (just one call to .half())\n",
        "- Works on both CPU and CUDA (best performance on GPU)\n",
        "- Good compromise between precision and performance\n",
        "- No retraining required\n",
        "- Can be combined with mixed precision training for best results\n",
        "\n",
        "## Mixed Precision\n",
        "\n",
        "Mixed precision is a technique where:\n",
        "- Some operations use FP32 (for stability and precision)\n",
        "- Most operations use FP16 (for speed and memory efficiency)\n",
        "- The model automatically determines which operations need higher precision\n"
      ],
      "metadata": {
        "id": "XmeK6A8D90nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. HALF PRECISION (FP16)\n",
        "# ----------------------\n",
        "print(\"\\n2. Converting to half precision (FP16)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Load a fresh model for half precision\n",
        "half_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "half_model.eval()\n",
        "\n",
        "# Convert to half precision\n",
        "half_model = half_model.half()  # This converts all parameters to float16\n",
        "fp16_conversion_time = time.time() - start_time\n",
        "\n",
        "# Save the half precision model\n",
        "half_precision_path = os.path.join(temp_dir, \"bert_sst2_fp16.pt\")\n",
        "torch.save(half_model.state_dict(), half_precision_path)\n",
        "half_precision_size_mb = get_file_size_mb(half_precision_path)\n",
        "\n",
        "print(f\"Half precision conversion time: {fp16_conversion_time:.2f} seconds\")\n",
        "print(f\"Half precision model size: {half_precision_size_mb:.2f} MB\")\n",
        "print(f\"Size reduction: {(1 - half_precision_size_mb/original_saved_size_mb) * 100:.2f}%\")\n",
        "\n",
        "# Evaluate half precision model\n",
        "half_model = half_model.to(device)\n",
        "half_precision_accuracy = evaluate_model(half_model, val_dataloader, device)\n",
        "print(f\"Half precision model accuracy: {half_precision_accuracy:.4f}\")\n",
        "\n",
        "half_precision_inference_time = measure_inference_time(half_model, sample_text, tokenizer, device)\n",
        "print(f\"Half precision model average inference time: {half_precision_inference_time:.2f} ms\")\n",
        "print(f\"Inference speedup: {baseline_inference_time / half_precision_inference_time:.2f}x\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHxzuqYHz2D0",
        "outputId": "75050548-e3ae-4f30-bc69-42c74c8ae603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Converting to half precision (FP16)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Half precision conversion time: 0.35 seconds\n",
            "Half precision model size: 208.90 MB\n",
            "Size reduction: 49.99%\n",
            "Half precision model accuracy: 0.5000\n",
            "Half precision model average inference time: 1975.05 ms\n",
            "Inference speedup: 0.21x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create summary table\n",
        "print(\"\\n===== SUMMARY =====\")\n",
        "print(\"Model                   | Size (MB) | Size Reduction | Inference Time (ms) | Accuracy\")\n",
        "print(\"------------------------|-----------|---------------|---------------------|----------\")\n",
        "print(f\"Original BERT           | {original_saved_size_mb:.2f}    | -             | {baseline_inference_time:.2f}              | {baseline_accuracy:.4f}\")\n",
        "print(f\"Dynamic Quantized       | {dynamic_quantized_size_mb:.2f}    | {(1 - dynamic_quantized_size_mb/original_saved_size_mb) * 100:.2f}%          | {dynamic_quantized_inference_time:.2f}              | {dynamic_quantized_accuracy:.4f}\")\n",
        "print(f\"Half Precision (FP16)   | {half_precision_size_mb:.2f}    | {(1 - half_precision_size_mb/original_saved_size_mb) * 100:.2f}%          | {half_precision_inference_time:.2f}              | {half_precision_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQOq4YFAz4F-",
        "outputId": "60e3a3eb-f364-49ba-9119-50446d94f7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SUMMARY =====\n",
            "Model                   | Size (MB) | Size Reduction | Inference Time (ms) | Accuracy\n",
            "------------------------|-----------|---------------|---------------------|----------\n",
            "Original BERT           | 417.72    | -             | 407.98              | 0.4821\n",
            "Dynamic Quantized       | 173.09    | 58.56%          | 216.08              | 0.5357\n",
            "Half Precision (FP16)   | 208.90    | 49.99%          | 1975.05              | 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save comparison results to JSON\n",
        "comparison_results = {\n",
        "    \"original_model\": {\n",
        "        \"parameter_count\": int(original_param_count),\n",
        "        \"size_mb\": float(original_saved_size_mb),\n",
        "        \"inference_time_ms\": float(baseline_inference_time),\n",
        "        \"accuracy\": float(baseline_accuracy)\n",
        "    },\n",
        "    \"dynamic_quantized\": {\n",
        "        \"size_mb\": float(dynamic_quantized_size_mb),\n",
        "        \"size_reduction_percent\": float((1 - dynamic_quantized_size_mb/original_saved_size_mb) * 100),\n",
        "        \"inference_time_ms\": float(dynamic_quantized_inference_time),\n",
        "        \"accuracy\": float(dynamic_quantized_accuracy)\n",
        "    },\n",
        "    \"half_precision\": {\n",
        "        \"size_mb\": float(half_precision_size_mb),\n",
        "        \"size_reduction_percent\": float((1 - half_precision_size_mb/original_saved_size_mb) * 100),\n",
        "        \"inference_time_ms\": float(half_precision_inference_time),\n",
        "        \"accuracy\": float(half_precision_accuracy)\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "bGyhbW1tz5dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_results_path = os.path.join(temp_dir, \"pytorch_quantization_comparison_results.json\")\n",
        "with open(comparison_results_path, 'w') as f:\n",
        "    json.dump(comparison_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nComparison results saved to: {comparison_results_path}\")\n",
        "print(f\"All models and artifacts saved to: {temp_dir}\")\n",
        "\n",
        "# Test models on example sentences\n",
        "test_sentences = [\n",
        "    \"I really enjoyed this movie, it was fantastic!\",\n",
        "    \"The restaurant was terrible and the service was slow.\",\n",
        "    \"The book was neither good nor bad, just average.\"\n",
        "]\n",
        "\n",
        "print(\"\\nTesting quantized models on example sentences:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvUsg2PFz8c8",
        "outputId": "018320af-57c8-4323-dc8d-f65e861d3a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison results saved to: /tmp/tmpzqcektnl/pytorch_quantization_comparison_results.json\n",
            "All models and artifacts saved to: /tmp/tmpzqcektnl\n",
            "\n",
            "Testing quantized models on example sentences:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYOCmG_ztWQ_",
        "outputId": "106afb48-56ea-4e9b-d8ff-fc3d3b505a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Dynamic Quantized Model:\n",
            "Text: 'I really enjoyed this movie, it was fantastic!'\n",
            "Prediction: Positive (confidence: 0.5008)\n",
            "Inference time: 354.82 ms\n",
            "--------------------------------------------------\n",
            "Text: 'The restaurant was terrible and the service was slow.'\n",
            "Prediction: Negative (confidence: 0.5028)\n",
            "Inference time: 221.48 ms\n",
            "--------------------------------------------------\n",
            "Text: 'The book was neither good nor bad, just average.'\n",
            "Prediction: Negative (confidence: 0.5071)\n",
            "Inference time: 214.73 ms\n",
            "--------------------------------------------------\n",
            "\n",
            "2. Half Precision Model:\n",
            "Text: 'I really enjoyed this movie, it was fantastic!'\n",
            "Prediction: Negative (confidence: 0.6641)\n",
            "Inference time: 1798.91 ms\n",
            "--------------------------------------------------\n",
            "Text: 'The restaurant was terrible and the service was slow.'\n",
            "Prediction: Negative (confidence: 0.6802)\n",
            "Inference time: 1754.50 ms\n",
            "--------------------------------------------------\n",
            "Text: 'The book was neither good nor bad, just average.'\n",
            "Prediction: Negative (confidence: 0.6626)\n",
            "Inference time: 1865.36 ms\n",
            "--------------------------------------------------\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n1. Dynamic Quantized Model:\")\n",
        "for sentence in test_sentences:\n",
        "    sentiment, confidence, inference_time = predict_sentiment(dynamic_quantized_model, sentence, tokenizer, device)\n",
        "    print(f\"Text: '{sentence}'\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\")\n",
        "    print(f\"Inference time: {inference_time:.2f} ms\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n2. Half Precision Model:\")\n",
        "for sentence in test_sentences:\n",
        "    sentiment, confidence, inference_time = predict_sentiment(half_model, sentence, tokenizer, device)\n",
        "    print(f\"Text: '{sentence}'\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {confidence:.4f})\")\n",
        "    print(f\"Inference time: {inference_time:.2f} ms\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nDone!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Quantization Techniques (Not Implemented)\n",
        "\n",
        "## FP8 Quantization\n",
        "- Emerging format for transformer models\n",
        "- Used in some NVIDIA GPUs (like H100)\n",
        "- Reduces precision to 8-bit floating point\n",
        "- Can achieve up to 4x memory reduction compared to FP32\n",
        "\n",
        "## INT4 Quantization\n",
        "- Ultra-low precision for weights only\n",
        "- Often used for inference-only scenarios\n",
        "- Requires careful handling of quantization parameters\n",
        "- Increasingly popular for LLMs in memory-constrained environments\n",
        "\n",
        "## Mixed-Precision Quantization\n",
        "- Different components of the model use different precisions\n",
        "- Example: attention layers in FP16, feed-forward networks in INT8\n",
        "- Can be optimized based on sensitivity analysis\n",
        "- Often yields the best accuracy/performance trade-off\n",
        "\n",
        "## Weight-Only Quantization\n",
        "- Only quantizes weight matrices, not activations\n",
        "- Common in transformer models where activation quantization causes accuracy issues\n",
        "- Less efficient than full quantization but more accurate\n",
        "- Used by techniques like GPTQ and AWQ for LLMs"
      ],
      "metadata": {
        "id": "UvvX7reu-A5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "- [PyTorch Implementation](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html)"
      ],
      "metadata": {
        "id": "atT9dBhvuDNi"
      }
    }
  ]
}